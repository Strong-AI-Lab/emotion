{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interspeech 2021 results\n",
    "This notebook displays the results for our Interspeech 2021 paper [1].\n",
    "\n",
    "[1] A. Keesing, Y. S. Koh, and M. Witbrock, ‘Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech’, in Interspeech 2021, Aug. 2021, pp. 3415–3419. doi: 10.21437/Interspeech.2021-2217."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from Orange.evaluation import graph_ranks\n",
    "from scipy.stats import friedmanchisquare, rankdata\n",
    "from statsmodels.stats.libqsturng import qsturng\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_friedman(table: pd.DataFrame, plot_cd: bool = True):\n",
    "    \"\"\"Runs Friedman test on given table and optionally graphs a\n",
    "    critical-difference diagram.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    table: DataFrame\n",
    "        The data table, with subjects as rows and independent variable\n",
    "        as columns.\n",
    "    \"\"\"\n",
    "    _, pvalue = friedmanchisquare(*table.transpose().to_numpy())\n",
    "    names = list(table.columns)\n",
    "    avgrank = rankdata(-table.to_numpy(), axis=1).mean(0)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Mean rank\": avgrank,\n",
    "            \"Mean\": table.mean(),\n",
    "            \"Std. dev.\": table.std(),\n",
    "            \"Median\": table.median(),\n",
    "            \"MAD\": table.mad(),\n",
    "        },\n",
    "        index=names,\n",
    "    ).sort_values(\"Mean rank\")\n",
    "\n",
    "    topclf = df.index[0]\n",
    "    n, k = table.shape\n",
    "    # Effect size is calculated in terms of differences in MAD\n",
    "    df[\"Effect size\"] = (df.loc[topclf, \"Median\"] - df[\"Median\"]) / np.sqrt(\n",
    "        ((n - 1) * df.loc[topclf, \"MAD\"] ** 2 + (n - 1) * df[\"MAD\"] ** 2) / (2 * n - 2)\n",
    "    )\n",
    "    cd = qsturng(0.95, k, np.inf) * np.sqrt((k * (k + 1)) / (12 * n))\n",
    "    print(f\"p = {pvalue}, cd = {cd:.2f} ranks\")\n",
    "    print(df.to_string())\n",
    "    print()\n",
    "    if plot_cd:\n",
    "        graph_ranks(avgrank, names, cd)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results using \"offline\" per-speaker normalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfs = []\n",
    "for results_csv in Path(\"./results/norm_offline\").glob(\"**/*.csv\"):\n",
    "    df = pd.read_csv(results_csv)\n",
    "    # Select columns before taking mean\n",
    "    df = df[[\"uar\", \"war\", \"microf1\", \"macrof1\"]].mean().to_frame().T\n",
    "    df[\"corpus\"] = results_csv.parts[-3]\n",
    "    df[\"clf\"] = results_csv.parts[-2]\n",
    "    df[\"features\"] = results_csv.stem\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_clf = df.pivot_table(values=\"uar\", index=\"corpus\", columns=\"clf\", aggfunc=\"max\")\n",
    "mean_clf = df.pivot_table(values=\"uar\", index=\"corpus\", columns=\"clf\", aggfunc=\"mean\")\n",
    "max_feat = df.pivot_table(\n",
    "    values=\"uar\", index=\"corpus\", columns=\"features\", aggfunc=\"max\"\n",
    ")\n",
    "mean_feat = df.pivot_table(\n",
    "    values=\"uar\", index=\"corpus\", columns=\"features\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "print(\"Friedman test for classifiers by corpus:\")\n",
    "run_friedman(mean_clf)\n",
    "avgrank = np.argsort(rankdata(-mean_clf.to_numpy(), axis=1).mean(0))\n",
    "rankclf = mean_clf.columns[avgrank]\n",
    "\n",
    "print(\"Friedman test for features by corpus:\")\n",
    "run_friedman(mean_feat)\n",
    "avgrank = np.argsort(rankdata(-mean_feat.to_numpy(), axis=1).mean(0))\n",
    "rankfeat = mean_feat.columns[avgrank]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_clf = max_clf[rankclf]\n",
    "mean_clf = mean_clf[rankclf]\n",
    "max_feat = max_feat[rankfeat]\n",
    "mean_feat = mean_feat[rankfeat]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(12, 7))\n",
    "sns.heatmap(max_clf, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[0])\n",
    "ax[0].set_title(\"Max UAR over features\")\n",
    "sns.heatmap(mean_clf, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[1])\n",
    "ax[1].set_title(\"Mean UAR over features\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(20, 7))\n",
    "sns.heatmap(max_feat, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[0])\n",
    "ax[0].set_title(\"Max UAR over classifiers\")\n",
    "sns.heatmap(mean_feat, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[1])\n",
    "ax[1].set_title(\"Mean UAR over classifiers\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results using \"online\" normalisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfs = []\n",
    "for results_csv in Path(\"./results/norm_online\").glob(\"**/*.csv\"):\n",
    "    df = pd.read_csv(results_csv)\n",
    "    # Select columns before taking mean\n",
    "    df = df[[\"uar\", \"war\", \"microf1\", \"macrof1\"]].mean().to_frame().T\n",
    "    df[\"corpus\"] = results_csv.parts[-3]\n",
    "    df[\"clf\"] = results_csv.parts[-2]\n",
    "    df[\"features\"] = results_csv.stem\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_clf = df.pivot_table(values=\"uar\", index=\"corpus\", columns=\"clf\", aggfunc=\"max\")\n",
    "mean_clf = df.pivot_table(values=\"uar\", index=\"corpus\", columns=\"clf\", aggfunc=\"mean\")\n",
    "max_feat = df.pivot_table(\n",
    "    values=\"uar\", index=\"corpus\", columns=\"features\", aggfunc=\"max\"\n",
    ")\n",
    "mean_feat = df.pivot_table(\n",
    "    values=\"uar\", index=\"corpus\", columns=\"features\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "print(\"Friedman test for classifiers by corpus:\")\n",
    "run_friedman(mean_clf)\n",
    "avgrank = np.argsort(rankdata(-mean_clf.to_numpy(), axis=1).mean(0))\n",
    "rankclf = mean_clf.columns[avgrank]\n",
    "\n",
    "print(\"Friedman test for features by corpus:\")\n",
    "run_friedman(mean_feat)\n",
    "avgrank = np.argsort(rankdata(-mean_feat.to_numpy(), axis=1).mean(0))\n",
    "rankfeat = mean_feat.columns[avgrank]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_clf = max_clf[rankclf]\n",
    "mean_clf = mean_clf[rankclf]\n",
    "max_feat = max_feat[rankfeat]\n",
    "mean_feat = mean_feat[rankfeat]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(12, 7))\n",
    "sns.heatmap(max_clf, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[0])\n",
    "ax[0].set_title(\"Max UAR over features\")\n",
    "sns.heatmap(mean_clf, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[1])\n",
    "ax[1].set_title(\"Mean UAR over features\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(20, 7))\n",
    "sns.heatmap(max_feat, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[0])\n",
    "ax[0].set_title(\"Max UAR over classifiers\")\n",
    "sns.heatmap(mean_feat, vmin=0, vmax=1, cmap=\"Blues\", annot=True, ax=ax[1])\n",
    "ax[1].set_title(\"Mean UAR over classifiers\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "a3f065a1863ba32ddd25f16970abb37688536e7ffb4990f693ec8f4a1356a969"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}